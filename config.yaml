bart:
  num_train_epochs: 5
  per_device_train_batch_size: 16
  per_device_eval_batch_size: 16
  logging_steps: 100
  save_strategy: 'no'
  learning_rate: 2e-5
  weight_decay: 0.01
  fp16: True

llama-2:
  num_train_epochs: 5
  per_device_train_batch_size: 8
  gradient_accumulation_steps: 2
  gradient_checkpointing: True
  optim: 'paged_adamw_32bit'
  logging_steps: 10
  save_strategy: 'no'
  learning_rate: 2e-4
  bf16: True
  fp16: False
  tf32: True
  max_grad_norm: 0.3
  warmup_ratio: 0.03
  lr_scheduler_type: 'constant'